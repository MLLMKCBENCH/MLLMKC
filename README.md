## Table of Contents

- [Table of Contents](#table-of-contents)
- [ğŸ”” News](#-news)
- [ğŸŒŸ Overview](#overview)
- [ğŸ¤— Dataset](#-dataset)
- [ğŸ› ï¸ Requirements and Installation](#ï¸-requirements-and-installation)
- [ğŸ’¥ Inference](#training)
- [ğŸ’¥ Evaluation](#training)


## ğŸ”” News


* **[2025.5.16]**  **Code** is available now!

* **[2025.5.16]**  We release the **MMKE-Bench dataset** at ğŸ¤— [Huggingface Dataset](https://huggingface.co/datasets/starjyf/MLLMKC-datasets).


## ğŸŒŸOverview

**TL;DR:** we introduce <span style="color:brown">**MLLMKC**</span>, a  Multi-Modal Knowledge Conflict benchmark, designed to analyze factual knowledge conflict under both context-memory and inter-context scenarios.

<img src="figs\overview.png" width="900px">

</p>

## ğŸ¤— Dataset

You can download **MMKC-Bench data** ğŸ¤— [Huggingface Dataset](https://huggingface.co/datasets/starjyf/MLLMKC-datasets). And the expected structure of files is:

```text
MLLMKC
|-- image
|   |-- nike
|   |-- kobe
|   |-- .....
|-- ER.json
|-- people_knowledge.json
|-- logo_knowledge.json
|-- IS.json
```

## ğŸ› ï¸ Requirements and Installation

```text
# clone MMKC-Bench
git clone https://github.com/MLLMKCBENCH/MLLMKC.git

# create conda env
cd MLLMKC
conda create -n mllmkc python=3.10
cd VLMEvalKit
pip install -r requirements.txt
```


## ğŸ’¥Inference
**Note: If you want to use local model weights, download them before running experiments:**, and in VLMEvalKit/vlmeval/config.py change the local weight path inside

**Begin to replace the following .sh file to revise the MODEL_NAME like "InternVL3-8B", name and VLMEvalKit vlmeval/config.py offer is consistent with the name of the file.**

For non-GPT models

**For the original answer:**
```shell
# Mutl-choise question format
bash start_original_mcq.sh

# Open-ended question answer format 
bash start_original_open.sh
```

**For the internal and external conflicts answer:**
```shell
# Mutl-choise question format
bash start_mcq_ie.sh

# Open-ended question answer format 
bash start_open_ie.sh
```

**For the external and external conflicts answer:**
```shell
# Mutl-choise question format
bash start_mcq_ee.sh

# Open-ended question answer format 
bash start_open_ee.sh
```

**For GPT models:**
```shell
bash start_gpt.sh
```

**For conflict detection:**
```shell
# Coarse-grained conflict detection
bash detection_coarse.sh

# Fine-grained conflict detection
bash detection_fine.sh
```


## ğŸ’¥Evaluationï¼š
We also provide the relevant code for the evaluation.We also provide the relevant code for the evaluation. Please check in detail: MLLMKC/evaluation/evaluation.py.

We need to organize the resulting file generated by the model into the following format,and take MODEL_OUT as input to the evaluation:

```text
MODEL_OUT
|-- original
|   |-- ER
|   |-- IS
|   |-- people_knowledge
|   |-- logo_knowledge
|-- output
|   |-- ER
|   |-- IS
|   |-- people_knowledge
|   |-- people_knowledge
```



    



