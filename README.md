## Table of Contents

- [Table of Contents](#table-of-contents)
- [🔔 News](#-news)
- [🌟 Overview](#overview)
- [🤗 Dataset](#-dataset)
- [🛠️ Requirements and Installation](#️-requirements-and-installation)
- [💥 Inference](#training)
- [💥 Evaluation](#training)


## 🔔 News


* **[2025.5.16]**  **Code** is available now!

* **[2025.5.16]**  We release the **MMKE-Bench dataset** at 🤗 [Huggingface Dataset](https://huggingface.co/datasets/starjyf/MLLMKC-datasets).


## 🌟Overview

**TL;DR:** we introduce <span style="color:brown">**MLLMKC**</span>, a  Multi-Modal Knowledge Conflict benchmark, designed to analyze factual knowledge conflict under both context-memory and inter-context scenarios.

<img src="figs\overview.png" width="900px">

</p>

## 🤗 Dataset

You can download **MMKC-Bench data** 🤗 [Huggingface Dataset](https://huggingface.co/datasets/starjyf/MLLMKC-datasets). And the expected structure of files is:

```text
MLLMKC
|-- image
|   |-- nike
|   |-- kobe
|   |-- .....
|-- ER.json
|-- people_knowledge.json
|-- logo_knowledge.json
|-- IS.json
```

## 🛠️ Requirements and Installation

```text
# clone MMKC-Bench
git clone https://github.com/MLLMKCBENCH/MLLMKC.git

# create conda env
cd MLLMKC
conda create -n mllmkc python=3.10
cd VLMEvalKit
pip install -r requirements.txt
```


## 💥Inference
**Note: If you want to use local model weights, download them before running experiments:**, and in VLMEvalKit/vlmeval/config.py change the local weight path inside

**Begin to replace the following .sh file to revise the MODEL_NAME like "InternVL3-8B", name and VLMEvalKit vlmeval/config.py offer is consistent with the name of the file.**

For non-GPT models

**For the original answer:**
```shell
# Mutl-choise question format
bash start_original_mcq.sh

# Open-ended question answer format 
bash start_original_open.sh
```

**For the internal and external conflicts answer:**
```shell
# Mutl-choise question format
bash start_mcq_ie.sh

# Open-ended question answer format 
bash start_open_ie.sh
```

**For the external and external conflicts answer:**
```shell
# Mutl-choise question format
bash start_mcq_ee.sh

# Open-ended question answer format 
bash start_open_ee.sh
```

**For GPT models:**
```shell
bash start_gpt.sh
```

**For conflict detection:**
```shell
# Coarse-grained conflict detection
bash detection_coarse.sh

# Fine-grained conflict detection
bash detection_fine.sh
```


## 💥Evaluation：
We also provide the relevant code for the evaluation.We also provide the relevant code for the evaluation. Please check in detail: MLLMKC/evaluation/evaluation.py.

We need to organize the resulting file generated by the model into the following format,and take MODEL_OUT as input to the evaluation:

```text
MODEL_OUT
|-- original
|   |-- ER
|   |-- IS
|   |-- people_knowledge
|   |-- logo_knowledge
|-- output
|   |-- ER
|   |-- IS
|   |-- people_knowledge
|   |-- people_knowledge
```



    



