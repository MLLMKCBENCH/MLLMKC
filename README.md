## Table of Contents

- [Table of Contents](#table-of-contents)
- [ğŸ”” News](#-news)
- [ğŸŒŸ Overview](#overview)
- [ğŸ¤— Dataset](#-dataset)
- [ğŸ› ï¸ Requirements and Installation](#ï¸-requirements-and-installation)
- [ğŸ’¥ Inference](#training)
- [ğŸ’¥ Evaluation](#training)
- [â­ Star History](#star-history)



## ğŸ”” News


* **[2025.5.16]**  **Code** is available now!

* **[2025.5.16]**  We release the **MMKE-Bench dataset** at ğŸ¤— [Huggingface Dataset](https://huggingface.co/datasets/starjyf/MLLMKC-datasets).




## ğŸŒŸOverview

**TL;DR:** We propose <span style="color:brown">**MLLMKC**</span>, a challenging benchmark for evaluating factual multi-modal knowledge conflict.

<img src="figs\overview.png" width="900px">


</p>

## ğŸ¤— Dataset

You can download **MMKC-Bench data** ğŸ¤— [Huggingface Dataset](https://huggingface.co/datasets/starjyf/MLLMKC-datasets). And the expected structure of files is:

```text
MLLMKC
|-- image
|   |-- nike
|   |-- kobe
|   |-- .....
|-- ER.json
|-- people_knowledge.json
|-- logo_knowledge.json
|-- IS.json
```

## ğŸ› ï¸ Requirements and Installation

```text
# clone MMKC-Bench
git clone https://github.com/MLLMKCBENCH/MLLMKC.git

# create conda env
cd MLLMKC
conda create -n mllmkc python=3.10
cd VLMEvalKit
pip install -r requirements.txt
```


## ğŸ’¥Inference
**If you want to use local model weights, download them ahead of time:** And in VLMEvalKit/vlmeval/config.py change local weight inside

**Began to replace sh file to review the MODEL_NAME like "InternVL3-8B", name and VLMEvalKit vlmeval/config.py offer is consistent with the name of the file.**

For non-GPT models

**For The original answer(mcq):**
```shell
bash start_original_mcq.sh
```

**For The internal and external conflicts answer(mcq):**
```shell
bash start_mcq_ie.sh
```

**For The external and external conflicts answer(mcq):**
```shell
bash start_mcq_ee.sh
```

**For The original answer(openqa):**
```shell
bash start_original_open.sh
```

**For The internal and external conflicts answer(openqa):**
```shell
bash start_open_ie.sh
```

**For The external and external conflicts answer(openqa):**
```shell
bash start_open_ee.sh
```

For GPT models

**For The external and external conflicts answer(openqa):**
```shell
bash start_gpt.sh
```

**For Coarse-grained conflict detection:**
```shell
bash detection.sh
```
**For fine-grained conflict detection:**
```shell
bash detection_xin.sh
```

## ğŸ’¥Evaluationï¼š
We also provide the relevant code for the evaluation.We also provide the relevant code for the evaluation. Please check in detail: MLLMKC/evaluation/evaluation.py.

We need to organize the resulting file generated by the model into the following format,and take MODEL_OUT as input to the evaluation:

```text
MODEL_OUT
|-- original
|   |-- ER
|   |-- IS
|   |-- people_knowledge
|   |-- logo_knowledge
|-- output
|   |-- ER
|   |-- IS
|   |-- people_knowledge
|   |-- people_knowledge
```
## â­Star History

[![Star History Chart](https://api.star-history.com/svg?repos=MLLMKCBENCH/MLLMKC&type=Date)](https://www.star-history.com/#MLLMKCBENCH/MLLMKC&Date)


    



